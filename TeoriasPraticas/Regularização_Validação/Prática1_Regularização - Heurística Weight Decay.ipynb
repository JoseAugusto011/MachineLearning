{"cells":[{"cell_type":"markdown","metadata":{"id":"sKPfuEnapDnU"},"source":["# <font color=\"darkblue\"> Prática 01: Regularização - Heurística Weight Decay </font>"]},{"cell_type":"markdown","metadata":{"id":"nKa5whanpntS"},"source":["**Objetivos:**\n","\n","\n","*   Testar a Regressão logística com outra base de dados\n","*   Implementar a heurística *weight decay* para regularização da função inferida pelo algoritmo de aprendizagem\n","\n","**Requisitos de execução:**\n","\n","\n","*   Upload dos arquivos *logisticregression.py* e *heart_failure_clinical_records_dataset.csv*"]},{"cell_type":"markdown","metadata":{"id":"OPCbV-Udr1Pz"},"source":["**Atividade 1:**\n","\n","1. Visitar a base de dados: https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\n","2. Carregar os dados do arquivo *heart_failure_clinical_records_dataset.csv* utilizando o pandas.\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZROjmwlFocyd"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","heart_data = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n","print(heart_data.head())"]},{"cell_type":"markdown","metadata":{"id":"O73AMlZRtOTU"},"source":["**Atividade 2:**\n","\n","1. Extrair os valores do *DataFrame* pandas e colocar nas variáveis\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sU9ugDeOtaHd"},"outputs":[],"source":["Features = ['time','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','serum_creatinine','age', 'high_blood_pressure', 'platelets', 'serum_sodium', 'sex', 'smoking']\n","\n","x = heart_data[Features].values\n","y = heart_data[\"DEATH_EVENT\"].values\n","y = [+1 if y_ == 1 else -1 for y_ in y]\n"]},{"cell_type":"markdown","metadata":{"id":"LWlIeFootavy"},"source":["**Atividade 3:**\n","\n","1. Separar os dados em conjunto de treinamento e teste\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CEvKox1tsWj"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","x_T, x_test, y_T, y_test = train_test_split(x,y, test_size=0.2, random_state=100)\n","\n","mms = MinMaxScaler()\n","mms.fit(x_T)\n","x_test = mms.transform(x_test)\n","x_T = mms.transform(x_T)\n","\n","print(\"Tamanho treinamento: \" + str(len(x_T)))\n","print(\"Tamanho teste: \" + str(len(x_test)))"]},{"cell_type":"markdown","metadata":{"id":"gGjfXNZ-tsor"},"source":["**Atividade 4:**\n","\n","1. Utilize a classe LogisticRegression, importada do pacote *sklearn.metrics*, para inferir aprendizado dos dados de treinamento;\n","2. Compute as métricas de aprendizado sobre os dados de teste."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvw7C1Odt3Jb"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","model = LogisticRegression(solver='newton-cg')\n","model.fit(x_T, y_T)\n","\n","print(classification_report(y_test, model.predict(x_test)))"]},{"cell_type":"markdown","metadata":{"id":"0SH_p2JeTmEN"},"source":["**Atividade 5:**\n","\n","1. Utilize a classe LogisticRegression, importada do arquivo *logisticregression.py*, para inferir aprendizado dos dados de treinamento;\n","2. Compute o $E_{in}$ sobre os dados de treino e o $E_{out}$ sobre os dados de teste;\n","2. Compute as métricas de aprendizado sobre os dados de teste."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbnZN9PUQQ0d"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from logisticregression import LogisticRegression\n","\n","lrY = [+1 if value == 1 else -1 for value in y_T]\n","lr_y_test = [+1 if value == 1 else -1 for value in y_test]\n","\n","#Regressão Logistica\n","classifier = LogisticRegression(0.1, 10000, 64)\n","classifier.fit(x_T, lrY)\n","\n","def error(pred, y):\n","    N = len(y)\n","    error = 0\n","    for i in range(N):\n","        if(pred[i] != y[i]):\n","          error += 1\n","    error /= N\n","    return error\n","\n","#Computando o erro dentro da amostra (Ein)\n","pred = classifier.predict(x_T)\n","print(\"Ein = \" + str(error(pred, lrY)))\n","\n","#Computando o erro fora da amostra (Eout)\n","pred = classifier.predict(x_test)\n","print(\"Eout = \" + str(error(pred, lr_y_test)))\n","\n","#Métricas de aprendizado\n","print(classification_report(lr_y_test, pred))"]},{"cell_type":"markdown","metadata":{"id":"7eMziL89QAyK"},"source":["Atividade 6:\n","\n","1. Implemente a heurística *WeightDecay* utilizando a penalidade $L_2$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZXxRdSyQBLX"},"outputs":[],"source":["from logisticregression import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","#import warnings\n","\n","\n","class WeightDecay:\n","    def __init__(self):\n","      self.w = None\n","      #warnings.filterwarnings(\"error\")\n","\n","    # Infere o vetor w da funçao hipotese\n","    #Executa a heuristica Weigth Decay\n","    def fit(self, _X, _y):\n","        #Quebrar o dataset em Treino e validação\n","        #Implementar a heurística\n","        #Setar em self.w a melhor hipotese avaliada usando o dataset de validação\n","\n","\n","\n","    #funcao hipotese inferida pela regressa logistica\n","    def predict_prob(self, X):\n","        return [(1 / (1 + np.exp(-(self.w[0] + self.w[1:].T @ x)))) for x in X]\n","\n","    #Predicao por classificação linear\n","    def predict(self, X):\n","        return [1 if (1 / (1 + np.exp(-(self.w[0] + self.w[1:].T @ x)))) >= 0.5\n","                else -1 for x in X]\n","\n","    def getW(self):\n","        return self.w\n","\n","    def getRegressionY(self, regressionX, shift=0):\n","        return (-self.w[0]+shift - self.w[1]*regressionX) / self.w[2]\n","\n","    def getEout(self, pred, y_val):\n","        #Computando o erro quadrático (Eout)\n","        N_val = len(y_val)\n","        eOut = 0\n","        for i in range(N_val):\n","            if(pred[i] != y_val[i]):\n","              eOut += 1\n","        eOut /= N_val\n","        return eOut"]},{"cell_type":"markdown","metadata":{"id":"Fyr1aWvJQGbI"},"source":["**Atividade 7:**\n","\n","1. Utilize a classe WeightDecay para inferir aprendizado dos dados de treinamento;\n","2. Compute o $E_{in}$ sobre os dados de treino e o $E_{out}$ sobre os dados de teste;\n","3. Compute as métricas de aprendizado sobre os dados de teste e compare os resutlados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGqo7qhUQHrY"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","#Weigth Decay heuristic\n","classifier = WeightDecay()\n","classifier.fit(x_T, lrY)\n","\n","#Computando o erro dentro da amostra (Ein)\n","pred = classifier.predict(x_T)\n","print(\"\\nEin = \" + str(error(pred, lrY)))\n","\n","#Computando o erro dentro da amostra (Eout)\n","pred = classifier.predict(x_test)\n","print(\"Eout = \" + str(error(pred, lr_y_test)))\n","\n","#Métricas de aprendizado\n","pred = classifier.predict(x_test)\n","print(classification_report(lr_y_test, pred))"]},{"cell_type":"markdown","metadata":{"id":"VI3GaycEKXh-"},"source":["Atividade 8:\n","\n","1. Implementar a validação da Logistic Regression através da classe *GridSearchCV* do pacote *sklearn.model_selection*;\n","2. Imprimir os parâmetros do melhor classificador;\n","3. Imprimir os $E_{in}$ e $E_{out}$  e as métricas de aprendizado.\n","\n","Parâmetros:\n","\n","\n","*   *estimator* : instância do classificador cujos hiperparâmetros serão analisados;\n","*   *cv* : número de divisões do conjunto de treinamento para ser usado na técnica de validação cruzada (10 é um bom valor observado na prática);\n","*   *param_grid* : conjunto de parâmetros a serem combinados durante a fase de validação.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elugBPN4DAWo"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report, accuracy_score\n","import numpy as np\n","\n","lr = LogisticRegression(solver='newton-cg')\n","\n","# Create the random grid\n","param_grid = {\n","              'C' : [10 ** x for x in range(-10, 10)],\n","              #'solver' : [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n","              'penalty': ['l2']\n","              }\n","\n","CV_rf = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 10, verbose=2, n_jobs=-1)\n","\n","# Fit the random search model\n","CV_rf.fit(x_T, y_T)\n","\n","print(CV_rf.best_estimator_)\n","\n","\n","print('Ein: %0.4f' % (1 - accuracy_score(y_T, CV_rf.predict(x_T))))\n","print('Eout: %0.4f' % (1 - accuracy_score(y_test, CV_rf.predict(x_test))))\n","print(classification_report(y_test, CV_rf.predict(x_test)))"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNX6QJ5a4nPEeMaYrPFuO52"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}