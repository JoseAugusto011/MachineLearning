{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geTopographic(topographicUrl):\n",
    "    \n",
    "    # receber o nome da cidade \n",
    "    # pega os dados de localização e altitude média  minimas e máximas\n",
    "    # retorna uma lista com os dados\n",
    "    \n",
    "   \n",
    "    print(topographicUrl)\n",
    "    r2 = requests.get(topographicUrl)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    \n",
    "    # Verificação de erro caso a cidade não exista no site \n",
    "    if r2.status_code == 404 or soup2 == None:\n",
    "        return 'Cidade não encontrada'\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # pegar todos os itens que posuuem <div>\n",
    "\n",
    "        info = soup2.find_all('div')\n",
    "        text = [i.text for i in info]\n",
    "\n",
    "        #pegar partes do tetxo que iniciam por 'Nome:'\n",
    "\n",
    "        data = [t for t in text if t.startswith('Nome:')]\n",
    "        \n",
    "        \"\"\"\n",
    "            #pegar os valores de localização e altitude média  minimas e máximas\n",
    "            data = data[-1].split(',')[-1]\n",
    "            #deletar string '\\xa0m'\n",
    "            data = data.replace('\\xa0m', '')\n",
    "            #separa ao encontrar letras maiusculas\n",
    "            data = re.findall('[A-Z][^A-Z]*', data)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        if data == []:\n",
    "            return [None, None, None, None]\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            data = data[-1].split(',')[-1]\n",
    "            data  = data.replace('\\xa0m', '')\n",
    "\n",
    "\n",
    "            #substitui Regex por split\n",
    "            l = data.split(')')[0].split('(')[-1]\n",
    "            print(\"L_>\",l)\n",
    "            aMed = data.split(')')[-1].split(':')[1].split('Altitude mínima')[0]\n",
    "            aMax= data.split(')')[-1].split(':')[-1].split('Altitude mínima')[0]\n",
    "            aMin = data.split(')')[-1].split(':')[2].split('Altitude máxima')[0]\n",
    "            \n",
    "            data = [l, aMed, aMax, aMin]\n",
    "            \n",
    "            return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetKoppenClassifyPt():\n",
    "    url = r'https://pt.wikipedia.org/wiki/Classifica%C3%A7%C3%A3o_clim%C3%A1tica_de_K%C3%B6ppen-Geiger'\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # get class 'mw-headline'\n",
    "    headline = soup.find_all('span', class_='mw-headline')\n",
    "    hlist = [h.text for h in headline]\n",
    "    hlist\n",
    "    # deixar apenas as classes que iniciam com Climas\n",
    "    hlist = [h for h in hlist if h.startswith('Clima')]\n",
    "\n",
    "    # capturar elemento <\\li> dos elementos de classe 'mw-headline' em hlist que estiverem com um ::marker\n",
    "    # get class 'mw-parser-output'\n",
    "\n",
    "    clines = soup.find_all('li')\n",
    "    #filtrar apenas os elementos que terminam com )\n",
    "    citilist = [c.text for c in clines if c.text.endswith(')')]\n",
    "    #deletar os ultimos 5 elementos da lista\n",
    "    citilist = citilist[:-5]\n",
    "    climate_dict = {} # chaves = hlist, valores = citilist\n",
    "\n",
    "    for h in hlist:\n",
    "        climate_dict[h] = [] #cria uma lista vazia para cada chave\n",
    "\n",
    "    for c in citilist:\n",
    "        cityCode = c[-4::].strip('()') # codigo da cidade\n",
    "        for h in hlist:\n",
    "            hCode = h.split('−')[1] # codigo do clima\n",
    "            if cityCode in hCode: # se o codigo da cidade estiver no codigo do clima\n",
    "                climate_dict[h].append(c)\n",
    "                \n",
    "    # criar dataframe com uma coluna para cidade e uma para o clima da respectiva cidade\n",
    "    df = pd.DataFrame(columns=['Cidade', 'Clima'])\n",
    "\n",
    "\n",
    "\n",
    "    for k, v in climate_dict.items():\n",
    "        temp = pd.DataFrame({'Cidade': v, 'Clima': k})\n",
    "        df =  pd.concat([df, temp], ignore_index=True)\n",
    "\n",
    "    # retirar o codigo da cidade do nome da cidade e adicionar a coluna cidade\n",
    "\n",
    "    df['Cidade'] =  df['Cidade'].apply(lambda x: x.split('(')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetKoppenClassifyEn():\n",
    "    \n",
    "    #Retornar um dataframe com as cidades e seus respectivos climas de acordo com a classificação de Köppen\n",
    "    \n",
    "    \n",
    "    r = requests.get('https://en.wikipedia.org/wiki/K%C3%B6ppen_climate_classification')\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # get class 'mw-headline'\n",
    "    Headline = soup.find_all('span', {'class': 'mw-headline'})\n",
    "    Hlist = [h.text for h in Headline]\n",
    "\n",
    "    Hlist = [h for h in Hlist if 'climat' in h[-8:-1]]\n",
    "    #eliminar elemnetos que iniciam com group\n",
    "    Hlist = [h for h in Hlist if 'Group' not in h[0:5]]\n",
    "    #retirrar 'Marine west coast climate',\n",
    "    Hlist.remove('Marine west coast climate')\n",
    "\n",
    "    #capturar elemento <\\li> dos elementos de classe 'mw-headline' em hlist que estiverem com um ::marker\n",
    "    # get class 'mw-parser-output'\n",
    "\n",
    "    Clines = soup.find_all('li')\n",
    "    #filtrar apenas os elementos que terminam com )\n",
    "    Citilist = [c.text for c in Clines if c.text.endswith(')')]\n",
    "    #deletar os ultimos 15 elementos da lista\n",
    "    Citilist = Citilist[:-21]\n",
    "    Climate_dict = {} # chaves = hlist, valores = citilist\n",
    "    codeClimate = [] # lista com os codigos do clima\n",
    "\n",
    "    for h in Hlist:\n",
    "        Climate_dict[h] = [] #cria uma lista vazia para cada chave\n",
    "\n",
    "    for c in Citilist:\n",
    "        separated = c.split('(')[1].split(')')[0].split(', ')\n",
    "        codeClimate.append(separated[0]) # lista com os codigos do clima\n",
    "    \n",
    "        \n",
    "        for h in Hlist:\n",
    "            HCode = h.split(\":\")[0] # codigo do clima\n",
    "            \n",
    "            if HCode == separated[0]:\n",
    "                Climate_dict[h].append(c) # adiciona o nome da cidade na lista do clima correspondente\n",
    "                \n",
    "    #criar um dataframe com os dados\n",
    "\n",
    "    # criar dataframe com uma coluna para cidade e uma para o clima da respectiva cidade\n",
    "    df = pd.DataFrame(columns=['Cidade', 'Clima'])\n",
    "\n",
    "\n",
    "\n",
    "    for k, v in Climate_dict.items():\n",
    "        temp = pd.DataFrame({'Cidade': v, 'Clima': k})\n",
    "        df =  pd.concat([df, temp], ignore_index=True)\n",
    "\n",
    "    # retirar o codigo da cidade do nome da cidade \n",
    "\n",
    "    df['Cidade'] = df['Cidade'].apply(lambda x: x.split('(')[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adicionar colunas de localização e altitude média  minimas e máximas\n",
    "\n",
    "# df['Localização'] = df['Cidade'].apply(lambda x: geTopographic(x)[0])\n",
    "# df['Altitude Média'] = df['Cidade'].apply(lambda x: geTopographic(x)[1])\n",
    "# df['Altitude Mínima'] = df['Cidade'].apply(lambda x: geTopographic(x)[2])\n",
    "# df['Altitude Máxima'] = df['Cidade'].apply(lambda x: geTopographic(x)[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Browser(city):\n",
    "\n",
    "\n",
    "        \n",
    "    # Configuração do ChromeDriver em modo headless\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    # Configuração do ChromeDriver\n",
    "    chrome_path = r'C:\\Users\\Usuário\\Downloads\\chromedriver-win32\\chromedriver.exe'   # Substitua pelo caminho do seu ChromeDriver\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path, options=chrome_options)\n",
    "\n",
    "    # Abre o site\n",
    "    driver.get(\"https://pt-br.topographic-map.com/\");\n",
    "\n",
    "    # Espera explicitamente até que a caixa de pesquisa esteja presente\n",
    "    try:\n",
    "        search_box = WebDriverWait(driver, 2).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//form[@title=\"Pesquisar um mapa topográfico\"]/input[@name=\"query\"]'))\n",
    "        )\n",
    "\n",
    "        # Limpar qualquer texto existente na caixa de pesquisa\n",
    "        search_box.clear()\n",
    "\n",
    "        # Enviar a consulta \"Beirute\"\n",
    "        search_box.send_keys(city)\n",
    "\n",
    "        # Enviar a tecla Enter\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Aguarde alguns segundos para os resultados serem exibidos\n",
    "        time.sleep(1)\n",
    "        #fazer scrape dos dados\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "\n",
    "    finally:\n",
    "      \n",
    "       \n",
    "        return geTopographic(driver.current_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTable(url):\n",
    "    #MAIS RAPIDO\n",
    "    # Configuração do ChromeDriver em modo headless\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "\n",
    "    # Configuração do ChromeDriver\n",
    "    chrome_path = r'C:\\Users\\Usuário\\Downloads\\chromedriver-win32\\chromedriver.exe'  # Substitua pelo caminho do seu ChromeDriver\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path, options=chrome_options)\n",
    "\n",
    "    driver.get(url);\n",
    "\n",
    "    # Obtenha o HTML da página após a execução de scripts\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Use o BeautifulSoup para analisar o HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Armazene referências aos elementos fora do loop\n",
    "    city_element = soup.find('div', {'class': 'col-10 m_city_name fs0'})\n",
    "    country_element = soup.find('div', {'class': 'col-12 city_country fs0'})\n",
    "    table = soup.find('table', {'class': 'climateTable'})\n",
    "\n",
    "    # Verifique se os elementos foram encontrados antes de acessá-los\n",
    "    city = city_element.text if city_element else None\n",
    "    country = country_element.text if country_element else None\n",
    "    print(city, country)\n",
    "\n",
    "    if table:\n",
    "        rows = table.find_all('tr')[1:]\n",
    "\n",
    "        # Criar listas para armazenar os dados\n",
    "        months, min_temps, max_temps, rainfalls, rain_days = [], [], [], [], []\n",
    "\n",
    "        # Iterar sobre as linhas e extrair os dados\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            \n",
    "            if len(columns) < 5:\n",
    "                continue\n",
    "            months.append(columns[0].text.strip())\n",
    "            min_temps.append(float(columns[1].text.strip()))\n",
    "            max_temps.append(float(columns[2].text.strip()))\n",
    "            rainfalls.append(float(columns[3].text.strip()))\n",
    "            rain_days.append(float(columns[4].text.strip()))\n",
    "\n",
    "        # Feche o navegador após a extração de dados\n",
    "        #driver.quit()\n",
    "\n",
    "        return city, min_temps, max_temps, rainfalls, rain_days\n",
    "\n",
    "    else:\n",
    "        # Se a tabela não foi encontrada, feche o navegador e retorne None\n",
    "        #driver.quit()\n",
    "        return None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetTable2(url):\n",
    "    \n",
    "    #MAIS RAPIDO\n",
    "    # Configuração do ChromeDriver em modo headless\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "\n",
    "    # Configuração do ChromeDriver\n",
    "    chrome_path = r'C:\\Users\\Usuário\\Downloads\\chromedriver-win32\\chromedriver.exe'  # Substitua pelo caminho do seu ChromeDriver\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path, options=chrome_options)\n",
    "  \n",
    "    driver.get(url)\n",
    "\n",
    "    # Obtenha o HTML da página após a execução de scripts\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Use o BeautifulSoup para analisar o HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Armazene referências aos elementos fora do loop\n",
    "    city_element = soup.find('div', {'class': 'col-10 m_city_name fs0'})\n",
    "    country_element = soup.find('div', {'class': 'col-12 city_country fs0'})\n",
    "    table = soup.find('table', {'class': 'climateTable'})\n",
    "    if table is None:\n",
    "        print('table is none')\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    \n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    # Obtenha o texto dos elementos fora do loop\n",
    "    city = city_element.text if city_element else None\n",
    "    country = country_element.text if country_element else None\n",
    "    print(city,country)\n",
    "    # Criar listas para armazenar os dados\n",
    "    months, min_temps, max_temps, rainfalls, rain_days = [], [], [], [], []\n",
    "\n",
    "    # Iterar sobre as linhas e extrair os dados\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        \n",
    "        if len(columns)!=5:\n",
    "            print(\"Erro\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "        months.append(columns[0].text.strip())\n",
    "        min_temps.append(float(columns[1].text.strip()))\n",
    "        max_temps.append(float(columns[2].text.strip()))\n",
    "        rainfalls.append(float(columns[3].text.strip()))\n",
    "        rain_days.append(float(columns[4].text.strip()))\n",
    "        \n",
    "\n",
    "    # Feche o navegador após a extração de dados\n",
    "    #driver.quit()\n",
    "\n",
    "    return city, min_temps, max_temps, rainfalls, rain_days\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "koppenData = GetKoppenClassifyEn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Adicionar ao df as colunas de localização e altitude média, mínima e máxima para cada cidade usando a função Browser\n",
    "\n",
    "# localizaçaõ = []\n",
    "# AltitudeMédia = []\n",
    "# AltitudeMínima = []\n",
    "# AltitudeMáxima = []\n",
    "\n",
    "# for c in df['Cidade']:\n",
    "    \n",
    "#     print(c)\n",
    "#     response = Browser(c)\n",
    "#     if response != 'Cidade não encontrada' and len(response) == 4:\n",
    "        \n",
    "#         localizaçaõ.append(response[0])\n",
    "#         AltitudeMédia.append(response[1])\n",
    "#         AltitudeMáxima.append(response[2])\n",
    "#         AltitudeMínima.append(response[3])\n",
    "        \n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         localizaçaõ.append(None)   \n",
    "#         AltitudeMédia.append(None) \n",
    "#         AltitudeMínima.append(None)\n",
    "#         AltitudeMáxima.append(None)\n",
    "     \n",
    "# df['Localização'] = localizaçaõ\n",
    "# df['Altitude Média'] = AltitudeMédia\n",
    "# df['Altitude Mínima'] = AltitudeMínima\n",
    "# df['Altitude Máxima'] = AltitudeMáxima\n",
    "\n",
    "# #descobrir qual cidade da erro e também diferença entre df['Cidade'] e CityList;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAIN\n",
    "\n",
    "# #Adicionar ao df as colunas de localização e altitude média, mínima e máxima para cada cidade usando a função Browser\n",
    "\n",
    "# localizaçaõ = []\n",
    "# AltitudeMédia = []\n",
    "# AltitudeMínima = []\n",
    "# AltitudeMáxima = []\n",
    "\n",
    "\n",
    "# mediaMinTemp = []\n",
    "# mediaMaxTemp = []\n",
    "# Precipitação = []\n",
    "# QtDiasChuva = []\n",
    "\n",
    "# df  = pd.DataFrame()\n",
    "# DicTable = {}\n",
    "\n",
    "# #1 - 3519 --> range(1,3520)\n",
    " \n",
    "# for i in range(17,21):\n",
    "#     print(i)\n",
    "#     r = GetTable2(f'https://worldweather.wmo.int/en/city.html?cityId={i}')\n",
    "#     print(r[0])\n",
    "#     DicTable[r[0]] = r[1:]\n",
    "\n",
    "\n",
    "\n",
    "# df['Cidade'] = DicTable.keys()\n",
    "# df['Cidade'] = df['Cidade'].str.replace('✚','')\n",
    "\n",
    "\n",
    "\n",
    "# for c in DicTable.keys():\n",
    "    \n",
    "    \n",
    "#     if c != None:\n",
    "        \n",
    "#         print(\"VAlor entrada:  \",c)\n",
    "#         c = c.replace('✚','')\n",
    "        \n",
    "#         response = Browser(c)\n",
    "#         if response != 'Cidade não encontrada' and len(response) == 4:\n",
    "            \n",
    "#             localizaçaõ.append(response[0])\n",
    "#             AltitudeMédia.append(response[1])\n",
    "#             AltitudeMáxima.append(response[2])\n",
    "#             AltitudeMínima.append(response[3])\n",
    "            \n",
    "            \n",
    "#             r = GetTable2(f'https://worldweather.wmo.int/en/city.html?cityId={i}')\n",
    "#             print('Valor Request',r[0])\n",
    "            \n",
    "#             if str(r[0]).replace('✚','') == c and mediaMaxTemp != None:\n",
    "                \n",
    "#                 mediaMinTemp.append(np.mean(r[1]))\n",
    "#                 mediaMaxTemp.append(np.mean(r[2]))\n",
    "#                 Precipitação.append(np.mean(r[3]))\n",
    "#                 QtDiasChuva.append(np.mean(r[4]))\n",
    "                \n",
    "#             else:\n",
    "                \n",
    "#                 print(\"interno\")\n",
    "#                 mediaMaxTemp.append(None)\n",
    "#                 mediaMinTemp.append(None)\n",
    "#                 Precipitação.append(None)\n",
    "#                 QtDiasChuva.append(None)\n",
    "                \n",
    "#         else:\n",
    "            \n",
    "#             print(\"Medio\")\n",
    "#             localizaçaõ.append(None)   \n",
    "#             AltitudeMédia.append(None) \n",
    "#             AltitudeMínima.append(None)\n",
    "#             AltitudeMáxima.append(None)\n",
    "            \n",
    "#             mediaMaxTemp.append(None)\n",
    "#             mediaMinTemp.append(None)\n",
    "#             Precipitação.append(None)\n",
    "#             QtDiasChuva.append(None)\n",
    "#     else:\n",
    "\n",
    "#             print(\"Externo\")\n",
    "#             localizaçaõ.append(None)   \n",
    "#             AltitudeMédia.append(None) \n",
    "#             AltitudeMínima.append(None)\n",
    "#             AltitudeMáxima.append(None)\n",
    "            \n",
    "#             mediaMaxTemp.append(None)\n",
    "#             mediaMinTemp.append(None)\n",
    "#             Precipitação.append(None)\n",
    "#             QtDiasChuva.append(None)\n",
    "                \n",
    "# df['Localização'] = localizaçaõ\n",
    "# df['Altitude Média'] = AltitudeMédia\n",
    "# df['Altitude Mínima'] = AltitudeMínima\n",
    "# df['Altitude Máxima'] = AltitudeMáxima\n",
    "# df['Média Mínima Temperatura'] = mediaMinTemp\n",
    "# df['Média Máxima Temperatura'] = mediaMaxTemp\n",
    "# df['Precipitação'] = Precipitação\n",
    "# df['Qt Dias Chuva'] = QtDiasChuva     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                Apia, Samoa \n",
       "1                      Balikpapan, Indonesia \n",
       "2                         Davao, Philippines \n",
       "3                       Easter Island, Chile \n",
       "4    Fort Lauderdale, Florida, United States \n",
       "5                         Georgetown, Guyana \n",
       "6                Hilo, Hawaii, United States \n",
       "7                   Honiara, Solomon Islands \n",
       "8           Innisfail, Queensland, Australia \n",
       "9                            Ishigaki, Japan \n",
       "Name: Cidade, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koppenData[0:10]['Cidade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuário\\AppData\\Local\\Temp\\ipykernel_20820\\851715862.py:11: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chrome_path, options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pt-br.topographic-map.com/map-4lm1gp/Apia/\n",
      "L_> -13.99446 -171.92491 -13.67446 -171.60491\n",
      "https://pt-br.topographic-map.com/map-cvkh3q/Balikpapan/\n",
      "L_> -1.28233 116.72880 -1.01330 117.02267\n",
      "https://pt-br.topographic-map.com/map-vxln51/Davao-City/\n",
      "L_> 6.95621 125.21759 7.58647 125.74485\n",
      "https://pt-br.topographic-map.com/map-f8j1kl/Ilha-de-P%C3%A1scoa/\n",
      "L_> -27.40200 -109.67958 -26.85500 -109.00299\n",
      "https://pt-br.topographic-map.com/map-ss4b18/Fort-Lauderdale/\n",
      "L_> 26.07961 -80.20364 26.21192 -80.08715\n",
      "https://pt-br.topographic-map.com/map-h3qdgt/City-of-Georgetown/\n",
      "L_> 6.76709 -58.17472 6.82755 -58.10527\n",
      "https://pt-br.topographic-map.com/map-hg3ntp/Hilo-CDP/\n",
      "L_> 19.62100 -155.18441 19.74913 -154.99044\n",
      "https://pt-br.topographic-map.com/\n",
      "https://pt-br.topographic-map.com/map-6xp5tf/Innisfail/\n",
      "L_> -17.56417 145.99114 -17.48417 146.07114\n",
      "https://pt-br.topographic-map.com/\n"
     ]
    }
   ],
   "source": [
    "#Para cada cidade de koppenData[\"Cidade\"] pegar os dados usando da função Browser e adicionar ao df\n",
    "y = []\n",
    "for cd in koppenData[0:10]['Cidade']:\n",
    "    y.append(Browser(cd))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-13.99446 -171.92491 -13.67446 -171.60491', ' 174', ' 1.148', ' 0'],\n",
       " ['-1.28233 116.72880 -1.01330 117.02267', ' 21', ' 129', ' -5'],\n",
       " ['6.95621 125.21759 7.58647 125.74485', ' 235', ' 2.627', ' -1'],\n",
       " ['-27.40200 -109.67958 -26.85500 -109.00299', ' 5', ' 501', ' 0'],\n",
       " ['26.07961 -80.20364 26.21192 -80.08715', ' 5', ' 38', ' -4'],\n",
       " ['6.76709 -58.17472 6.82755 -58.10527', ' 2', ' 11', ' -3'],\n",
       " ['19.62100 -155.18441 19.74913 -154.99044', ' 193', ' 728', ' 0'],\n",
       " [None, None, None, None],\n",
       " ['-17.56417 145.99114 -17.48417 146.07114', ' 12', ' 233', ' -2'],\n",
       " [None, None, None, None]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cidade</th>\n",
       "      <th>Clima</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apia, Samoa</td>\n",
       "      <td>Af: Tropical rainforest climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balikpapan, Indonesia</td>\n",
       "      <td>Af: Tropical rainforest climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Davao, Philippines</td>\n",
       "      <td>Af: Tropical rainforest climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Easter Island, Chile</td>\n",
       "      <td>Af: Tropical rainforest climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fort Lauderdale, Florida, United States</td>\n",
       "      <td>Af: Tropical rainforest climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>Scott Base, Antarctica</td>\n",
       "      <td>EF: Ice cap climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Showa Station, Antarctica</td>\n",
       "      <td>EF: Ice cap climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Summit Camp, Greenland, Denmark</td>\n",
       "      <td>EF: Ice cap climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Ushakov Island, Russia</td>\n",
       "      <td>EF: Ice cap climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Vostok Station, Antarctica</td>\n",
       "      <td>EF: Ice cap climate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Cidade                            Clima\n",
       "0                                Apia, Samoa   Af: Tropical rainforest climate\n",
       "1                      Balikpapan, Indonesia   Af: Tropical rainforest climate\n",
       "2                         Davao, Philippines   Af: Tropical rainforest climate\n",
       "3                       Easter Island, Chile   Af: Tropical rainforest climate\n",
       "4    Fort Lauderdale, Florida, United States   Af: Tropical rainforest climate\n",
       "..                                        ...                              ...\n",
       "370                   Scott Base, Antarctica               EF: Ice cap climate\n",
       "371                Showa Station, Antarctica               EF: Ice cap climate\n",
       "372          Summit Camp, Greenland, Denmark               EF: Ice cap climate\n",
       "373                   Ushakov Island, Russia               EF: Ice cap climate\n",
       "374               Vostok Station, Antarctica               EF: Ice cap climate\n",
       "\n",
       "[375 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koppenData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7eff589202d1b081309a21c1fcab32e7bd6b92ee77a4b4c9e08aff7652052eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
